
 в Озоне, в Даноне и в Насле.
 По сути, внедрял управленку, помогал бизнесу именно с бизнес-аналитикой и всякие вот подобные связанные вещи делал.
 Конкретно сейчас я больше как внешний консультант выступаю, то есть у меня тоже своя компания небольшая, пока с одним мной в этой компании.
 И я под ключ, получается, делаю три продукта.
 Первый – это помогаю предпринимателям экономить деньги, там просто вот фокусно под ключ, как вот именно продукт.
 Второе – это разрабатываю финансовые модели для прогнозирования и для привлечения инвестиций.
 А третий продукт – это как раз внедряю управленческую отчетность.
 Это если совсем вкратце, не уходя в дебри, понятно, что могу рассказать про какие-то кейсы, еще что-то, но я думаю, это сейчас… Отличный ответ, да-да, спасибо.
 Как раз исходя из этого ответа тогда вопросы будут направлены, наверное, не только на твой личный опыт, но и на опыт компаний, бизнесов, которые помогал с данными, с внедрением управленческого отчета и так далее.
 И следующий вопрос, ну, получается, ты работаешь с данными и твои клиенты тоже работают с данными, верно? Да, все так.
 А можешь рассказать, как работаешь с данными, как вообще работа устроена у тебя и у твоих клиентов? Окей, у меня, по сути, такая небольшая ремарка будет, что, по сути, раньше у меня было все организовано так, что, по сути, был один клиент, которого я веду от и до.
 И последние клиенты, с которыми я работал, имели самописные решения как раз по системе управленческой отчетности, в которых нужно было при помощи SQL вытаскивать запросами определенные линии затрат и всякое такое для того, чтобы потом в последующем в Power BI вертеть это все в дэшбордах.
 Это если вот так вот совсем, грубо говоря, да, по еще предыдущему опыту, если мы говорим про Danone, ну, я думаю, Евгений знает, как никто другой, как там все было организовано.
 То есть, грубо говоря, был основной модуль, где бухгалтерская отчетность была, это WhatsApp.
 Дальше, соответственно, была развернута именно управленка детализированная по продуктам в Oracle кубах, где, соответственно, бизнес имел возможность с глубокой детализацией вытаскивать определенные данные.
 Потом уже самостоятельно докручивал их в Excel, либо строил какие-то модели на базе того, что там было уже непосредственно, либо тянул напрямую опять же в тот же Power BI или в микростратегию.
 Еще у меня был опыт с одним клиентом с компанией Avon, у них тоже самописное решение, и, соответственно, там получается все то же самое, что есть именно некая бухгалтерская своя разработка, и оттуда вытаскиваются какие-то определенные данные, строчки, которые требуются бизнесу для отчетности.
 И дополнительно есть еще ряд именно программ для бизнес-аналитики типа того же самого Power BI, который используется для визуализации.
 Ну вот если совсем прямо грубо, то вот как-то так.
 Я понял.
 Скажи, Денис, а с какими проблемами и трудностями часто сталкивался ты и твои клиенты, реализи данных? Ну для меня на самом деле первый вопрос был, это когда я хотел очень глубокую детализацию выкачать, я вот часто помню еще и с Женей в Danone с этим разбирался, что были некие лимиты на то, сколько ты сразу можешь вытащить данных.
 То есть, грубо говоря, объясню, зачем это нужно.
 Иногда бизнес требует возможность в одном месте, грубо говоря, иметь аналитику, которая позволит максимально глубоко проваливаться именно через структуру.
 Для того, чтобы это все обеспечить, нужно иметь некий Data Lake, который будет как некая прокладка между самим решением, тем же самым Oracle кубом, допустим, и пользователем.
 Для того, чтобы бесшовно можно было выгружать быстро эти данные и вертеть больше миллионам строк в моменте.
 Вот с этим на самом деле огромный геморрой всегда был, чтобы была возможность беспрепятственно, относительно быстро оперировать всеми данными, которые есть.
 Вторая история была, на самом деле, иногда с коннекторами беда.
 То есть, условно, некоторые данные приходится периодически перетаскивать руками, что-то колдовать каким-то образом для того, чтобы этот переток обеспечить.
 На это просто время затрачивается и, как правило, это не очень дешево.
 Поэтому периодически определенную структуру данных нужно создавать в Excel через некие прокладки и потом доливать.
 И это тоже не очень удобно.
 Третий, наверное, вопрос – это больше к интерфейсу.
 Если мы говорим по кубы, то к ним у меня вопросов нет.
 Я понимаю, что не все используют развернутые кубы.
 Есть опять же внутренние разработки, где черт ногу сломит о том, как вообще этим пользоваться.
 И даже если с этим разберешься, то это очень неудобная сама по себе платформа.
 Даже у меня к ЦАПу иногда есть вопросы по тому, где что находится.
 И там очень перегруженный интерфейс.
 Я понял.
 Спасибо.
 А как пытался ты или твой клиент эти трудности решить? Ну смотри, по поводу первой трудности как раз через Data Lake было интегрирование решения для того, чтобы был некий промежуточный сервис-хранилище, к которому можно было подключаться и вытаскивать эти данные.
 В принципе, таким образом как раз и решилось все.
 А второе решение – это когда на стороне Power BI там тоже был развернутый сервер, насколько я помню.
 То есть там была большая лицензия, у Ozone как раз было такое.
 И на стороне Power BI было хранение как раз некой массивной информации, грубо говоря, некого слепка за 2 часа назад.
 И каждые 2-3 часа данные подгружались как раз на вот этот некий промежуточный сервер, с которого уже брались для Power BI, для более глубокой аналитики.
 Ну там на самом деле с Ozone вообще отдельная история, потому что у них по большому счету из-за вот этих вот собственных разработок есть плюсы, есть минусы.
 Там огромный бардак с точки зрения управленческой отчетности.
 И что там собственные их решения развернутые периодически дают сбой, все ломается.
 И там, например, был прецедент, когда на 4 месяца вообще выбило всю управленческую отчетность по всей компании.
 Просто потому что у них накрылся сервер, короче, и айтишники там долго ковыряли, пытались понять, что там с ними не так.
 В общем, были вопросы.
 По ощущениям, наверное, много компания потратила денег на то, чтобы собственно решение сделать.
 Ну, в принципе, да, есть, конечно, сейчас плюс такой, что по счастливой случайности после того, как всякие там вендоры вроде того же самого SAP ушли из России, на них это никак не отразилось.
 И в принципе большинство вот этих детских проблем, которые были, они вроде закрыли.
 Но, тем не менее.
 И плюс еще, кстати, тоже вот так вот накинуть, если по поводу проблемы.
 Я замечаю, что есть клиенты крупных компаний, по опыту, я заметил, у которых очень много всяких инструментов развернутых.
 Просто охренеть сколько, которые плюс-минус дублируют друг друга.
 Ну, типа непонятно зачем экономически.
 То есть, грубо говоря, компания не жалеет средств, да, в принципе у них там все неплохо было.
 Но в то же время я понимаю, как финансист, что это не особо рентабельно.
 То есть от силы пользуются, может быть, один-два инструмента.
 И вот это вот проблема засады именно крупных компаний, потому что на мелких там каждая копейка на счету, а крупные не всегда как бы сильно прицельно… Да, да, да.
 То есть им лишь бы идти в ногу со временем, а иногда бывает такое, что по сути у тебя сто-пятьсот этих одинаковых вообще приложений непонятно для чего сделанных.
 В общем, на мой взгляд это неэффективно нифига.
 Ну да.
 Окей.
 Дальше у меня есть три блока вопросов, направленные на такую проблематику.
 С нашей точки зрения она есть, и поэтому мы попытаемся выяснить, есть она вообще или нет на твоем опыте.
 Давай.
 Расскажи, приходится ли тебе работать с данными из разных источников? С данными из разных источников, да? Да.
 Да, все так.
 Как правило, вообще, если я делаю какую-то аналитику или модель, то это минимум где-то пять-десять источников, которые нужно друг другом там скрещивать и совмещать.
 Это как раз второй вопрос.
 С какими трудностями сталкиваешься, когда вот пять-десять источников? Слушай, ну первое, это, наверное, неконсистентный данных.
 Второе, это такой камень в сторону айтишников.
 Иногда расхождение в самих данных.
 То есть я смотрю данные из разных систем и они отличаются.
 Ну и начинаю раскапывать именно, что же там такое случилось.
 Обнаруживаю, что там какие-то проблемы при перетоке произошли или там какие-то разные методологии были при внедрении.
 Ну, короче, разные причины могут быть.
 Ну и получается так, что поскольку мне аналитика нужна, как правило, здесь сейчас, то я на коленке делаю какой-то костыль для того, чтобы адаптировать эти данные сами по себе.
 Это вот прям классическая, мне кажется, боль всех финансистов, потому что, условно, менеджмент хочет сейчас ему посрать, что у него там платформа упала, условно.
 Поэтому это всегда чревато вот такими interim solutions.
 Низ, а часто вообще-то происходит, когда несколько источников и вот такая проблема возникает? Каждый раз или бывает на банусе? Слушай, ну на самом деле, по моему опыту во всех компаниях, где я работал, со всеми… У меня сейчас не было очень много клиентов, потому что это… Видите, пропал.
 Отключился.
 Да, сейчас слышно? Да, да, слышно.
 Вернулся.
 Да, случайно нажал на кнопку.
 Короче, я пока не встречал компаний, кроме, может быть, Nestle, в которой в разных источниках одни и те же данные прям полностью идеально совпадали с первой попытки.
 Ну то есть в любых компаниях, где я бы работал, либо там у меня они были как клиенты, пока, может быть, не так много у меня клиентов было, но, тем не менее, вот пока везде такая проблема есть.
 Да, они, может быть, разлетаются там нематериально, условно, ну то есть там меньше процента, что плюс-минус считается допустимым, но это вызывает вопросы определенные.
 Тем более ты никогда не знаешь, этот один процент – это минус 6 плюс 5 условно, или это просто один процент, и тратишь на это время.
 Ну что, в принципе, мне кажется, должно явно решаться каким-то образом на уровне IT решение и значения.
 А, ну и еще, кстати, тоже в верницу проблем.
 Это, я думаю, про работоспособность.
 Это прям классическая история.
 То есть то, сколько времени тебе нужно для того, чтобы извлечь данные какие-то.
 Ну то есть, условно, сама выгрузка и само взаимодействие с системой.
 То есть, окей, тебе действительно нужно иногда это слеплять на своей стороне, но бывает еще и история такая, что, чтобы, в принципе, тебе сформировать такую аналитику, тебе нужно потратить определенное время на какие-то выгрузки в разных форматах и так далее, что тоже, конечно, не круто.
 А ты сказал, какие-то костыли пытался сам делать, чтобы все эти данные из разных источников собрать в одном месте и чтобы они были релеванты друг к другу.
 Какие костыли обычно используешь? Слушай, ну самое простое, что можно сделать, это там на базе XLA что-нибудь такое навертеть.
 Если чуть-чуть посложнее, то это там на базе Power Query можно что-нибудь сделать.
 То есть посмотреть, чем данные друг от друга отличаются по структуре и привести их к единому знаменателю через mapping.
 Третий вариант – это как раз попробовать развернуть решение как раз на базе Power BI того же самого.
 Это то, что по сути в Ozone было реализовано, когда было несколько промежуточных вот этих вот витрин, про которые я говорил, и они при помощи запросов на питание собирали данные из разных как раз подсистем, микросервисов.
 И формировали некую базу данных.
 И потом, соответственно, эта большая база данных подгружалась в сервер Power BI для последующего визуализации.
 Ну, как бы просто проблема, что если что-нибудь там сломается, то потом, соответственно, тратишь время на то, чтобы условно это починить.
 И иногда бывает такое, что приходится делать вручную все вещи, которые должны быть системами делать.
 Хорошо.
 Денис, скажи, а приходилось тебе или твоим клиентам со стравочниками работать? Да, конечно.
 Это прям регулярно.
 Окей.
 А как процесс устроен? Ну и там тоже про трудности расскажи.
 Слушай, процесс… Самое банальное то, что я знаю, это когда есть некий мэппинг в условном Excel, который заполняется как такая большая таблица, потом отправляется в какую-то систему, и, соответственно, в этой системе постоянно используется.
 Это вот прям самое банальное, что я помню.
 Есть какие-то сложности со справочниками, с которыми ты или клиенты остались столкнувшись? Слушай, со справочниками плюс-минус нету.
 Единственное, просто опять же, со справочниками, наверное, с точки зрения больше процессов внутри компании и вопроса, что должна явно быть какая-то структура, которая берет на себя ответственность для того, чтобы за ними следить, чтобы они были все актуализированы.
 Особенно если этих справочников много.
 Самое классное – это условно иметь какую-то единую мастер дату, которая бы подливалась во все именно подсистемы.
 И использовать один, грубо говоря, источник, а не множество.
 Потому что зачастую бывает такое, что параллельно существует 6 мастер дат, которые с одними и теми же данными, а в одних кто-нибудь что-нибудь забудет обновить.
 И привет.
 То есть правильно услышал, что в основном в трупных компаниях, в которых ты работал, у твоих клиентов и у тебя лично со справочниками проблем особо не возникают? Да, в принципе как таковых супер драматичных проблем нет.
 Но бывают случаи, когда параллельно существует несколько справочников и они не интегрированы в один.
 То есть если мы говорим про разворачивание какой-нибудь управленческой отчетности, то первый шаг здесь – это иметь какой-то нормальный лаконичный справочник, который был бы единственным центром входа, который бы кем-то администрировался процессно.
 И это была единственная, истинно последняя инстанция, чтобы не было каких-то параллельных версий.
 А если у вас сталкивается с тем, что справочников несколько и они между собой не согласовываются? Как тогда получится? Хорошо.
 Следующий блок у меня будет про сводные таблицы.
 Приходится ли тебе или твоим клиентам с сводными таблицами работать? Да, конечно.
 Расскажи тоже, что обычно используете и с какими капностями сталкиваетесь? На уровне сводных просто удобно смотреть вот эту декомпозицию с погружением, когда ты можешь между уровнями передвигаться.
 Ну и, соответственно, вертеть тем, чтобы у тебя то в шапке, то в строчке передвигались именно данные, которые ты смотришь.
 Ну и, соответственно, еще в сводниках мне нравится, когда можно группировать.
 Бывают иногда те данные, которые изначально системно заданы как разношерстные, а там по-быстрому их можно сгруппировать так, чтобы справочники какие-то не городить.
 И конкретно для своей небольшой аналитики по-быстрому их сгруппировать.
 Это удобная функция.
 Плюс еще бывает, когда вычисляемые какие-то колонки тоже делаются.
 Но это отдельная история.
 Скажи, а при работе с сводными таблицами какие трудности бывают? Бывают ли вообще? Слушайте, да, в принципе, наверное, нет особых трудностей.
 Там как бы те же вопросы, которые и предыдущие бывают.
 Если сами данные на старте так себе, то и работа с сводной таблицей несколько затруднена.
 А так, в принципе, это очень простой сам по себе инструмент.
 Там особо, мне кажется, нет каких-то трудностей.
 А трудности есть при загрузке данных для формирования сводных отчетов? Ты тоже, наверное, ответил на этот вопрос, что если данные не очень, то это долго.
 Тут смотри, какой момент.
 Если у меня есть возможность подключиться к этим данным напрямую, ну, или через какой-то интерфейс, о чем я говорил, то в принципе проблем нет.
 Проблема есть тогда, когда, по сути, нет промежуточного хранилища а-ля некого Data Lake, который… Я, кстати, не знаю, в данном ли в итоге внедрили его.
 Я уже к тому моменту не работал в компании.
 Все там хотели, хотели это сделать, да, в итоге, по-моему, так и не сделали.
 Сейчас там уже не рассказывают.
 Ну да, это к Жене, наверное, вопрос больше.
 И просто когда нет вот этой истории, то последующая аналитика предполагает, что нужно где-то кэшировать вот эти слепки, какие-то XLI собирать у себя или еще что-то такое.
 Ну, понятно, что ты больше на это время не тратишь.
 Если у тебя есть уже, грубо говоря, есть несколько этапов работы с управленкой.
 Первый – это когда она сама по себе собирается правильно в самой системе, грубо говоря, и в нормальном интерфейсе.
 А второй следующий – это когда ты на базе учетной системы, некого условного 1С, пытаешься делать какие-то аналитики.
 И вот там самое офигенное, что может быть, это если у тебя есть условный SQL сервер или еще что-то, к чему ты можешь подключиться и вытащить эти данные условно в самом нижнем уровне.
 Не обязательно, что они именно как слепок представлены.
 Это могут быть и эти, как на вертике, знакомы с таким решением, где отдельно колонки существуют, грубо говоря, и данные вообще не присутствуют в неком собранном виде.
 Но тем же питоном можно собирать их опять же воедино.
 Отвалился.
 Отвалился, да.
 Классно, чувак.
 Вообще классно.
 Вообще про все в тему.
 Теперь я слышу, что есть проблемы.
 Денис? Алло.
 Да, да, да.
 Ты как раз рассказывал.
 Я говорил, что в принципе основное крутится вокруг того, чтобы хранить.
 Грубо говоря, есть несколько потребностей, с которыми сталкивается бизнес, когда у них есть учетная система.
 Первая задача – это сформировать сами данные.
 Грубо говоря, чтобы в принципе управленческая отчетность посчиталась корректно.
 А второе – это сделать саму аналитику и условно посчитать какие-то отходки, еще что-то такое.
 И вот для второй задачи тут уже важно иметь опять же какое-то некое типа хранилище, которое позволит прямым вот этим подключением к данным финансистам или бизнесу вытащить какие-то нужные метрики и собственно самостоятельно собрать через Power BI или еще через что-то какие-то отчеты.
 Я говорил, что это может быть как и какая-то плоская таблица, да, там типа на SQL сервере сделанная, либо это может быть как вот в вертике отдельно по колонкам расформированные данные.
 Просто не у всех финансистов высокие навыки по Python.
 Там как раз нужно немножко с пупом потанцевать для того, чтобы это все объединить.
 Но, тем не менее.
 Супер, супер.
 Окей.
 Скажи, Денис, может быть, ты можешь посоветовать кого-нибудь еще со сложными трудностями, с кем мы тоже могли бы поговорить для нашего исследования? Слушай, да, наверное, да.
 Могу пару коллег закинуть, можно с ними пообщаться.
 Как раз вот есть ребята из… один мой знакомый работает в лаборатории Касперского, Кирилл Калыванов, там Евгений Яросюк знает его.
 Он, в принципе, тоже, наверное, работает достаточно много с управленкой, поэтому интересно, что вот он скажет.
 Ну и еще, я думаю, будет пара ребят, о которых могу порекомендовать, с которыми тоже можно будет пообщаться.
 Отлично, мы были бы очень благодарны.
 Да, да.
 Окей.
 Хорошо, Денис, и последний вопрос, который у меня остался.
 Есть ли какие-то вопросы, которые не следовало бы задать о том, о чем мы только что говорили? Может, я что-то забыл упустить? Можешь повторить еще разочек? Есть какие-то еще вопросы, которые не следовало бы задать на эту тему, которую мы только что обсудили? Правда, полчаса.
 Может быть, я что-то забыл упустить? Слушай, ну, так, наверное, прям с лету, если то нет.
 Единственное… Может, какие-то еще уголи есть? В каких, может быть, программным продуктам требуются какие-то интерфейсы? Вот что-нибудь такое, например, если так понакинуть.
 Ну, то есть можно составить а-ля список самых популярных программ, с которыми взаимодействуют как раз в бизнесах.
 Но я думаю, у вас уже накидан подобный.
 Можно просто такой SWOT составить еще и по другим ребятам, по другим компаниям для того, чтобы условно он был такой исчерпывающий а-ля.
 Чтобы можно было продумать опять же структуру связи.
 Да, я понял, понял.
 Конкретно по моему опыту это, ну, абсолютно точно, это как раз Power BI тот же самый, да.
 Потом это может быть табло.
 Ну, то есть, грубо говоря, это вот эти вот визуализирующие инструменты, которые существуют.
 Это может быть… что там еще? Это могут быть всякие MP-статы, вот эти вот все маркетинговые программы для аналитики.
 Ну, то есть, грубо говоря, есть определенный перечень вот этих вот программ, которые явно нужно будет проработать.
 Не знаю, на каком этапе он у вас есть, но условно, чтобы можно было ко всему подключаться.
 Ко всему можно было интегрироваться.
 Да, да, да, да.
 Вот это было бы очень удобно, потому что когда все в одном месте, то гораздо проще.
 Спасибо большое.
 Вот так в принципе, наверное, нет проблем.
 Очень ценный вопрос.
 
